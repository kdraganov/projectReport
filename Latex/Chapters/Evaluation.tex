%Evaluates the artefact, comparing results expected from theory (including those for alternative designs) with those obtained in practice.
\chapter{Results \& Evaluation}
Evaluation is important step of every software project yet it is sometime neglected. For our project we  are only ably to objectively evaluate the performance of the disruption detection engine. Evaluation of the visualisation is very subjective and it will vary greatly between one user and another. For this reason we only rely on feedback from users especially CentreComm operator which we can use as a measure for the success of the particular visualisation we have implemented to complete this project. The disruption engine however needs to be properly evaluated. In order to evaluate it we need to answer three questions:
\begin{itemize}
	\item What is being evaluated?
	\item How is it evaluated?
	\item What are the results have been obtained?
\end{itemize}
\section{Engine}
AS DISCUSSED IN THE BACKGROUND THERE ARE NO STUDIES DONE ON USING AVL DATA FOR DETECTING CONGESTION ON ARTERIAL URBAN TRANSPORT NETWORK

EXPLAIN THAT WE HAVE CREATED A SIMULATION FEED THREAD FOR COPYING THE FEEDS

To answer the first question we need to measure the ability of the prototype to detect accurately and in real-time disruptions in the network (this is one of the main aims of the project). This means that we need to consider two aspects:
\begin{enumerate}
	\item Is the tool able to detect all disruptions that need to be detected?
	\item Are those disruption detected in real-time?
\end{enumerate}

In order to check if the system we have proposed can satisfy those requirements we need to run simulations and check if they result generated by the prototype match our expectations. One option to accomplish this is to use real data that was provided by TSG of TFL. However the problem with this approach is that we do not have a complete list of what was the actual state of the network at that time (the period of the sample data provided). It would be infeasible for us to manually analyse it in order to calculate what, when and for how long it should be picked up. This leads us to the seconds option which is to manually generate some test data which conforms to the iBus AVL data.

We have limited our test data to four scenarios as follows:
\begin{enumerate}
	\item The schedule deviation value remain fairly constant with very minor changes across the route for every bus on that route. This scenarios should yield no disruption detections.
	\item Single bus incident (e.g. bus breakdown, customer incident, error reading etc.). This again should not be picked up by the system.
	\item General traffic scenario. This means that there is gradual schedule deviation increase at some point of the route run. This is the most typical real life scenario (e.g. rush hour traffic build up) and it should be detected by the system.
	\item Incident (e.g. traffic collision) along a route. The data for this would represent a sudden increase in the schedule deviation. This needs to be detected by the tool.
\end{enumerate}

Once we have generated the data for the above scenarios we need to run the system and feed it with the artificially generated input. We run each of the above scenarios separately and the system is re-initialised before each run. This allows us to have the system in the exactly same state for each test.

This tests allowed us to run multiple simulation in order to adjust the weighted moving average parameters. These are the weights and the moving average window. This parameter values are critical for the accuracy of our tool. They have direct impact on what is being detected by the tool and with what lag. We also altered the data validity parameter which represents when we discard any observations.

In order to calibrate our prototype and its output we ran a numerous simulation with the describer scenarios and different values for our parameters. Using our test generated data we have obtained best results with having the data validity set to 90 minutes, but then only have moving average window of 5. This means that we only consider the last 5 observations for a given section which have occurred in the last hour and a half. We also use weight of 1 to 5 (1 for the oldest and 5 for the most recent). This allowed for a balance output, as the system tended to overreact if we used an exponentially growing weights or it lagged behind if we used a greater moving average window. Also keeping data for the last 90 minutes in memory should work well for both low and high frequency buses, as from the data provided we do not have any indication of the type of the service.

\section{Visualisation}
The user interface 
Usability


%Include evaluation strategy walk through. Sample data used.
%Compare the output with the actual data by calculating the error (the difference between the prediction and the actual value). Sum of the squared errors (SSE) and the mean of the squared errors (MSE). The model/values that minimize these are best.
%Averaging methods: These techniques could be evaluated by calculating the error (the difference between the prediction and the actual value), the squared error and also the the sum of the squared errors (SSE) and respectively the mean of the squared errors (MSE).
%The model which minimizes the MSE is the best. It can be shown mathematically that the one that minimizes the MSE for a set of random data is the mean.
%Consider metrics for Mean Absolute Difference and Mean Absolute Error. Peak analysis.
%Determining the optima Weights and the data window size.
%root mean square error (RMSE) and root mean square percentage error (RMSPE)