\chapter{Results \& Evaluation}
Evaluates the artefact, comparing results expected from theory (including those for alternative designs) with those obtained in practice.
\section{Evaluation}
Include evaluation strategy walk through. Sample data used.
Compare the output with the actual data by calculating the error (the difference between the prediction and the actual value). Sum of the squared errors (SSE) and the mean of the squared errors (MSE). The model/values that minimize these are best.
%Averaging methods: These techniques could be evaluated by calculating the error (the difference between the prediction and the actual value), the squared error and also the the sum of the squared errors (SSE) and respectively the mean of the squared errors (MSE).
%The model which minimizes the MSE is the best. It can be shown mathematically that the one that minimizes the MSE for a set of random data is the mean.
%Consider metrics for Mean Absolute Difference and Mean Absolute Error. Peak analysis.
%Determining the optima Weights and the data window size.
%root mean square error (RMSE) and root mean square percentage error (RMSPE)
\section{Results}