%Evaluates the artefact, comparing results expected from theory (including those for alternative designs) with those obtained in practice.
\chapter{Evaluation \& Results}
Evaluation is important step of every software project yet it is sometimes neglected. This chapter aims to discuss how and what evaluation has been done of the prototypical tool we have proposed and implemented. This is followed by discussing the results that were obtained during the evaluation.

%For our project we are only ably to objectively evaluate the performance of the disruption detection engine. Evaluation of the visualisation is very subjective and it will vary greatly between one user and another. For this reason we only rely on feedback from users especially CentreComm operator which we can use as a measure for the success of the particular visualisation we have implemented to complete this project. The disruption engine however needs to be properly evaluated. In order to evaluate it we need to answer three questions:
%What is being evaluated?
%How is it evaluated?
%What are the results have been obtained?
\section{Evaluation}

\subsection{Disruption Engine}
In order to evaluate and verify that our tool is able to detect disruptions we need to run the system with some data for which we know what the output should be. This means that we can analyse the output once the system has processed the sample input data. Analysing the data need to focus on whether the system is able to:
\begin{itemize}
	\item Detect all disruptions present in the data it is fed with.
	\item Detect and alert of those disruptions in real time (there is no or very little lag).
	\item Calculating accurate and reliable estimates of what is the actual (real life) state of the bus network.
\end{itemize}

However achieving this is problematic in our case as the data provided by TFL is only the real AVL feed files.
There is no formal information or list of all delays, their severity and duration for a given period of time. We have been provided with some links to web blogs which contain some of the diversions that are being implemented on daily basis in response to disruptions in the network. However, we cannot formally use this data to evaluate our product as this is neither an exhaustive list nor precise and accurate source of information (the delays stated there vary greatly and are somewhat subjective). We can analyse the provided bus feeds manually for a given period to extract what the actual state of the network was during that time. However, if we want to cover a number of scenarios this approach becomes infeasible as it will require great effort and time which are limited. And if we want more cases for evaluation it will require us to go over and repeat the same process again and again.

In order to overcome this problem we have decided to generate some test data on our own. We have achieved this by implementing a simple program for generating iBus AVL-like feed files. This allows us to compile test data with different scenarios and test if our system achieves the expected results. We have limited our test data to four scenarios as follows:
\begin{enumerate}
	\item The schedule deviation value remain fairly constant with very minor changes across the route for every bus on that route. This scenarios should yield no disruption detections.
	\item Single bus incident (e.g. bus breakdown, customer incident, error reading etc.). This again should not be picked up by the system.
	\item General traffic scenario. This means that there is gradual schedule deviation increase at some point of the route run. This is the most typical real life scenario (e.g. rush hour traffic build up) and it should be detected by the system.
	\item Incident (e.g. traffic collision) along a route. The data for this would represent a sudden increase in the schedule deviation. This needs to be detected by the tool.
\end{enumerate}

Once we have generated the data for the above scenarios, we need to run the system and feed it with the artificially generated input. We run each of the above scenarios separately and the system is re-initialised before each run. This allows us to have the system in the exactly same state for each test.

These tests allowed us to run multiple simulation in order to adjust the weighted moving average parameters. These are the weights and the moving average window. This parameter values are critical for the accuracy of our tool. They have direct impact on what is being detected by the tool and with what lag. We also altered the data validity parameter which represents when we discard any observations.

In order to calibrate our prototype and its output we ran a numerous simulations with the describer scenarios and different values for our parameters. Using our test-generated data we have obtained best results with having the data validity set to 90 minutes and the moving average window size equal to 5. This means that we only consider the last 5 observations for a given section which have occurred in the last hour and a half. We also use weight of 1 to 5 (1 for the oldest and 5 for the most recent). This allowed for a balance output, as the system tended to overreact if we used an exponentially growing weights or it lagged behind if we used a greater moving average window. Also keeping data for the last 90 minutes in memory should work well for both low- and high-frequency buses, as from the data provided we do not have any indication of the type of the service. This however, needs to be further tested and evaluated by either generating further test data or using real AVL data for which the bus network state at that point is know.

%In order to check if the system we have proposed can satisfy those requirements we need to run simulations and check if they result generated by the prototype match our expectations. One option to accomplish this is to use real data that was provided by TSG of TFL. However the problem with this approach is that we do not have a complete list of what was the actual state of the network at that time (the period of the sample data provided). It would be infeasible for us to manually analyse it in order to calculate what, when and for how long it should be picked up. This leads us to the seconds option which is to manually generate some test data which conforms to the iBus AVL data.

\subsection{Visualisation}
The web application part of our system was evaluated by simply using the list of requirements. Using the defined requirements from Chapter 3 of this report we were able to verify that all expected functionality is in place and produces the correct results. This also included testing that the application is behaving and displaying the same way on the most widely used browsers (Internet Explorer, Firefox and Chrome) as well one some mobile devices (Android tablet, Ipad and Iphone). All of the test verified that the system is consistent across the various devices and no functional issues have been identified.

\section{Results}
During the testing and evaluation of the proposed prototype we have ensured that all user and functional requirements have been met. Our simulations using artificially generated data proved that the proposed system is capable of effectively monitor changes in the schedule deviation value. However, further evaluations and analysis is required into whether using this values provide us with an accurate and reliable measure of the actual delays in the bus network. As it has been discussed in the background chapter of this report there is very little or no studies which address the problems this particular project is trying to solve. This means that we are unable to compare our results with those of others simply because we could not find any. 

%In this report we have identified some possible issues that can arise from the use of the schedule deviation as a measure.
%However as more evaluation need to be done whether the taken approach would accurate and reliable enough to be user in real life...

%AS DISCUSSED IN THE BACKGROUND THERE ARE NO STUDIES DONE ON USING AVL DATA FOR DETECTING CONGESTION ON ARTERIAL URBAN TRANSPORT NETWORK

%Include evaluation strategy walk through. Sample data used.
%Compare the output with the actual data by calculating the error (the difference between the prediction and the actual value). Sum of the squared errors (SSE) and the mean of the squared errors (MSE). The model/values that minimize these are best.
%Averaging methods: These techniques could be evaluated by calculating the error (the difference between the prediction and the actual value), the squared error and also the the sum of the squared errors (SSE) and respectively the mean of the squared errors (MSE).
%The model which minimizes the MSE is the best. It can be shown mathematically that the one that minimizes the MSE for a set of random data is the mean.
%Consider metrics for Mean Absolute Difference and Mean Absolute Error. Peak analysis.
%Determining the optima Weights and the data window size.
%root mean square error (RMSE) and root mean square percentage error (RMSPE)